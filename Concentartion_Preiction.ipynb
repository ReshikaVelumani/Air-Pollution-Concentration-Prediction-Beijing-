{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# for distance\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "# For model\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# For csv\n",
    "from itertools import cycle\n",
    "#######################################################################################################################\n",
    "#COMMON FUNCTIONS FOR ALL DATA\n",
    "#DATE_HRS Dataframe\n",
    "# Creating a dataframe with the time range\n",
    "def Date_hrs(min_date, max_date):\n",
    "    DATE_hrs = pd.DataFrame({\"date\": pd.date_range(min_date, max_date, freq='H')})\n",
    "    print(\"No of Hrs : \" +str(DATE_hrs.shape))\n",
    "    return DATE_hrs\n",
    "\n",
    "# Reading all the stations file only with lat, long, station name\n",
    "# Air quality station\n",
    "AQ_station = pd.read_csv(\"Beijing_AirQuality_Stations_en.csv\")\n",
    "# Observed weather station\n",
    "OW_station = pd.read_csv(\"OW_station_data.csv\")\n",
    "# Grid weather station\n",
    "GW_station = pd.read_csv(\"Beijing_grid_weather_station.csv\", names = [\"Grid_station\",\"latitude\",\"longitude\"])\n",
    "GW_station = GW_station.rename(columns={'Grid_station': 'station'})\n",
    "\n",
    "#Nearest station\n",
    "# Finiding nearest station\n",
    "def haversine_dist(long1, lat1, long2, lat2):\n",
    "    long1, lat1, long2, lat2 = map(radians, [long1, lat1, long2, lat2])\n",
    "    # haversine formula \n",
    "    dist_long = long2 - long1 \n",
    "    dist_lat = lat2 - lat1 \n",
    "    a = sin(dist_lat/2)**2 + cos(lat1) * cos(lat2) * sin(dist_long/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 \n",
    "    return c * r\n",
    "\n",
    "def near_station(lat, long, station):\n",
    "    dists = station.apply(lambda row: haversine_dist(lat, long, row['latitude'], row['longitude']), axis=1)\n",
    "    dist = dists[dists!=0]\n",
    "    return station.loc[dist.idxmin(), 'station']\n",
    "# Nearest AQ_station for AQ_station\n",
    "AQ_station[\"nearest_AQ\"] = AQ_station.apply(\n",
    "    lambda row: near_station(row['latitude'], row['longitude'],AQ_station), \n",
    "    axis=1)\n",
    "# Nearest OW_station for OW_station\n",
    "OW_station[\"nearest_OW\"] = OW_station.apply(\n",
    "    lambda row: near_station(row['latitude'], row['longitude'], OW_station), \n",
    "    axis=1)\n",
    "# Nearest GW_station for GW_station\n",
    "GW_station[\"nearest_GW\"] = GW_station.apply(\n",
    "    lambda row: near_station(row['latitude'], row['longitude'], GW_station), \n",
    "    axis=1)\n",
    "# Nearest OW_station for AQ_data\n",
    "Final_station = AQ_station\n",
    "Final_station[\"nearest_OW\"] = Final_station.apply(\n",
    "    lambda row: near_station(row['latitude'], row['longitude'], OW_station), \n",
    "    axis=1)\n",
    "# Nearest GW_station for AQ_data\n",
    "Final_station[\"nearest_GW\"] = Final_station.apply(\n",
    "    lambda row: near_station(row['latitude'], row['longitude'], GW_station), \n",
    "    axis=1)\n",
    "\n",
    "#Getting Year, Day, Month, Hour from Date\n",
    "# Getting day month hour week from time in new_df\n",
    "def ymd(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['year'], df['month'] ,df['hour'], df[\"week\"], df[\"day\"] = df['date'].dt.year, df['date'].dt.month, df['date'].dt.hour, df['date'].dt.week ,df['date'].dt.day\n",
    "    return df\n",
    "\n",
    "###############################################################################################################################################################################################################################################################\n",
    "###############################################################################################################################################################################################################################################################\n",
    "\n",
    "# TRAIN DATA\n",
    "# AIR QULAITY DATA\n",
    "\n",
    "# Reading the air quality data\n",
    "# Reading the air quality data\n",
    "AQ_jan17_to_jan18= pd.read_csv('airQuality_201701-201801.csv')\n",
    "AQ_feb18_march18= pd.read_csv('airQuality_201802-201803.csv')\n",
    "AQ_april18= pd.read_csv('aiqQuality_201804.csv')\n",
    "AQ_april18= AQ_april18.drop(['id'], axis=1)\n",
    "AQ_april18.columns = ['stationId','utc_time','PM2.5','PM10','NO2','CO', 'O3', 'SO2']\n",
    "\n",
    "# Appending all the air quality data\n",
    "AQ_data = pd.DataFrame()\n",
    "AQ_data = AQ_jan17_to_jan18.append([AQ_feb18_march18,AQ_april18])\n",
    "AQ_data = AQ_data.rename(index=str, columns={\"stationId\": \"station_id\", \"utc_time\": \"time\"})\n",
    "AQ_data[\"time\"] = pd.to_datetime(AQ_data['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Checking the shape of AQ_data before removing duplicates\n",
    "print(\"Shape of AQ_data before removing the duplicates:\", AQ_data.shape)\n",
    "# Dropping duplicates\n",
    "AQ_data.drop_duplicates(subset= None, keep= \"first\", inplace= True)\n",
    "# Sorting the entire dataframe based on station_id and time\n",
    "AQ_data = AQ_data.sort_values(by=['station_id', 'time'], ascending=[True,True] )\n",
    "# Checking the shape of AQ_data after removing duplicates\n",
    "print(\"Shape of AQ_data after removing the duplicates:\", AQ_data.shape)\n",
    "\n",
    "# train data\n",
    "# Getting range of time for air quality data\n",
    "min_date_train = AQ_data.time.min()\n",
    "max_date_train = AQ_data.time.max()\n",
    "# Creating DATE_HRS dataframe\n",
    "Date_hrs_train = Date_hrs(min_date_train, max_date_train)\n",
    "# since we need to train on all the air_quality data we using the same time frame for all the data\n",
    "\n",
    "# Performing cartesian product of station and days_hrs dataframes\n",
    "AQ_days_stations = pd.merge(Date_hrs_train.assign(key=0), AQ_station.assign(key=0), on='key').drop('key', axis=1)\n",
    "print(\"Station * Days\" + str(AQ_days_stations.shape))\n",
    "# Joining the air_quality and days_stations dataframes\n",
    "AQ_df = pd.merge(AQ_data, AQ_days_stations, how='right', left_on=['station_id','time'], \n",
    "                 right_on = ['station','date']).drop(['station_id','time'], axis =1)\n",
    "print(AQ_df.columns)\n",
    "AQ_df = AQ_df.sort_values(by=['date', 'station'], ascending=[True,True] )\n",
    "\n",
    "# Making copy of new_df\n",
    "AQ_nearstation = AQ_df\n",
    "# Renaming all the columns of df_copy\n",
    "AQ_nearstation = AQ_nearstation.drop(['nearest_AQ','nearest_OW','nearest_GW','longitude', 'latitude', 'type'], axis=1)\n",
    "AQ_nearstation.rename(columns={'PM2.5': 'n_PM2.5','PM10': 'n_PM10', \"NO2\":\"n_NO2\",\"CO\":\"n_CO\",\"O3\":\"n_O3\",\n",
    "                        \"SO2\":\"n_SO2\", \"date\":\"n_date\", \"station\":\"n_station\" }, inplace=True)\n",
    "# Appending the new_df and df_copy based on nearest station and time\n",
    "AQ_nearstation2 = pd.merge(AQ_df, AQ_nearstation, how='inner', left_on=['nearest_AQ','date'], \n",
    "                           right_on = ['n_station','n_date']).drop(['n_station','n_date', ], axis=1)\n",
    "# Sorting by date and station\n",
    "AQ_nearstation2 = AQ_nearstation2.sort_values(by=['station', 'date'], ascending=[True,True] )\n",
    "# Sorting the index\n",
    "AQ_nearstation2.sort_index(inplace= True)\n",
    "\n",
    "# Filling the NaN values\n",
    "# Finding the null values in each columns\n",
    "print(\"Null values before preprocessing\\n\", AQ_nearstation2.applymap(lambda x: pd.isnull(x)).sum())\n",
    "# since the zhiwuyuan_aq station has many missing values for more than a month we are assuming that the station is not \n",
    "# funcioning and we are filling it with zero\n",
    "AQ_nearstation2.loc[AQ_nearstation2['station'] == \"zhiwuyuan_aq\",['PM2.5','PM10','NO2','CO','O3','SO2']] = 0\n",
    "# Filling missing values with nearest station value\n",
    "AQ_nearstation2['PM10'].fillna(AQ_nearstation2['n_PM10'], inplace=True)\n",
    "AQ_nearstation2['PM2.5'].fillna(AQ_nearstation2['n_PM2.5'], inplace=True)\n",
    "AQ_nearstation2['NO2'].fillna(AQ_nearstation2['n_NO2'], inplace=True)\n",
    "AQ_nearstation2['CO'].fillna(AQ_nearstation2['n_CO'], inplace=True)\n",
    "AQ_nearstation2['O3'].fillna(AQ_nearstation2['n_O3'], inplace=True)\n",
    "AQ_nearstation2['SO2'].fillna(AQ_nearstation2['n_SO2'], inplace=True)\n",
    "AQ_nearstation2 = ymd(AQ_nearstation2)\n",
    "# Filling missing values in AQ data with the previous hour data of particular station, year, week\n",
    "AQ_nearstation2[['PM2.5','PM10','NO2','CO','O3','SO2']] = AQ_nearstation2.groupby([\"station\",\"year\",\"week\",\"hour\"])[['PM2.5', 'PM10', 'NO2', 'CO', 'O3', 'SO2']].transform(lambda x: x.ffill().bfill())\n",
    "# # Filling missing values with mean of station , month, hour\n",
    "AQ_nearstation2[['PM2.5','PM10','NO2','CO','O3','SO2']] = AQ_nearstation2.groupby([\"station\",\"hour\"])[['PM2.5', 'PM10', 'NO2', 'CO', 'O3', 'SO2']].transform(lambda x: x.fillna(x.mean()))\n",
    "print(\"Null values after preprocessing\\n\", AQ_nearstation2.applymap(lambda x:pd.isnull(x)).sum())\n",
    "AQ_final = AQ_nearstation2.drop(['n_PM2.5', 'n_PM10', 'n_NO2', 'n_CO', 'n_O3','n_SO2', 'year','week', 'day', 'nearest_AQ'],axis=1)\n",
    "\n",
    "###############################################################################################################################################################################################################################################################\n",
    "\n",
    "#OBSERVED WEATHER DATA\n",
    "\n",
    "# reading observed weather data\n",
    "obs_wea_jan17_jan18 = pd.read_csv(\"observedWeather_201701-201801.csv\")\n",
    "obs_wea_feb18_mar18 = pd.read_csv(\"observedWeather_201802-201803.csv\")\n",
    "obs_wea_apr18 = pd.read_csv(\"observedWeather_201804.csv\")\n",
    "obs_wea_apr18.columns = ['id','station_id', 'utc_time', 'weather', 'temperature', 'pressure',\n",
    "       'humidity', 'wind_speed', 'wind_direction']\n",
    "#appending the all observed weather data\n",
    "OW_data = pd.DataFrame()\n",
    "OW_data = obs_wea_jan17_jan18.append([obs_wea_feb18_mar18, obs_wea_apr18])\n",
    "OW_data = OW_data.drop([\"id\",\"weather\",\"latitude\",\"longitude\"], axis =1)\n",
    "OW_data = OW_data.rename(columns={\"utc_time\": \"time\"})\n",
    "OW_data[\"time\"] = pd.to_datetime(OW_data['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "# Checking the shape of AQ_data before removing duplicates\n",
    "print(\"Shape of OW_data before removing the duplicates:\", OW_data.shape)\n",
    "# Dropping duplicates\n",
    "OW_data.drop_duplicates(subset= None, keep= \"first\", inplace= True)\n",
    "# Sorting the entire dataframe based on station_id and time\n",
    "OW_data = OW_data.sort_values(by=['station_id', 'time'], ascending=[True,True] )\n",
    "# Checking the shape of OW_data after removing duplicates\n",
    "print(\"Shape of OW_data after removing the duplicates:\", OW_data.shape)\n",
    "\n",
    "# Performing cartesian product of station and days_hrs dataframes\n",
    "OW_days_stations = pd.merge(Date_hrs_train.assign(key=0), OW_station.assign(key=0), on='key').drop('key', axis=1)\n",
    "print(\"Station * Days\" + str(AQ_days_stations.shape))\n",
    "# Joining the air_quality and days_stations dataframes\n",
    "OW_df = pd.merge(OW_data, OW_days_stations, how='right', left_on=['station_id','time'], \n",
    "                 right_on = ['station','date']).drop(['station_id','time'], axis =1)\n",
    "OW_df = OW_df.sort_values(by=['date', 'station'], ascending=[True,True] )\n",
    "\n",
    "# Making copy of new_df\n",
    "OW_nearstation = OW_df\n",
    "# Renaming all the columns of df_copy\n",
    "OW_nearstation = OW_nearstation.drop(['nearest_OW','longitude', 'latitude'], axis=1)\n",
    "OW_nearstation.rename(columns={'humidity': 'n_humidity','pressure': 'n_pressure', \"temperature\":\"n_temperature\",\n",
    "                               \"wind_direction\":\"n_wind_direction\",\"date\":\"n_date\", \"wind_speed\":\"n_wind_speed\", \n",
    "                               \"station\":\"n_station\"}, inplace=True)\n",
    "# Appending the new_df and df_copy based on nearest station and time\n",
    "OW_nearstation2 = pd.merge(OW_df, OW_nearstation, how='inner', left_on=['nearest_OW','date'], \n",
    "                           right_on = ['n_station','n_date']).drop(['n_station','n_date'], axis=1)\n",
    "# Sorting by date and station\n",
    "OW_nearstation2 = OW_nearstation2.sort_values(by=['station', 'date'], ascending=[True,True] )\n",
    "# Sorting the index\n",
    "OW_nearstation2.sort_index(inplace= True)\n",
    "\n",
    "# Filling the NaN values\n",
    "\n",
    "# Finding the null values in each columns\n",
    "print(\"Null values before preprocessing\\n\", OW_nearstation2.applymap(lambda x: pd.isnull(x)).sum())\n",
    "\n",
    "# Filling missing values with nearest station value\n",
    "OW_nearstation2['humidity'].fillna(OW_nearstation2['n_humidity'], inplace=True)\n",
    "OW_nearstation2['pressure'].fillna(OW_nearstation2['n_pressure'], inplace=True)\n",
    "OW_nearstation2['temperature'].fillna(OW_nearstation2['n_temperature'], inplace=True)\n",
    "OW_nearstation2['wind_direction'].fillna(OW_nearstation2['n_wind_direction'], inplace=True)\n",
    "OW_nearstation2['wind_speed'].fillna(OW_nearstation2['n_wind_speed'], inplace=True)\n",
    "OW_nearstation2 = ymd(OW_nearstation2)\n",
    "# filling missing values in OW data with the previous hour data of particular station, year, week\n",
    "OW_nearstation2[['humidity','pressure','temperature','wind_direction','wind_speed']] = OW_nearstation2.groupby([\"station\",\"year\",\"week\",\"hour\"])[['humidity','pressure','temperature','wind_direction','wind_speed']].transform(lambda x: x.ffill().bfill())\n",
    "# Filling missing values with mean of station , month, hour\n",
    "OW_nearstation2[['humidity','pressure','temperature','wind_direction','wind_speed']] = OW_nearstation2.groupby([\"station\",\"hour\"])[['humidity','pressure','temperature','wind_direction','wind_speed']].transform(lambda x: x.fillna(x.mean()))\n",
    "print(\"Null values after preprocessing\\n\", OW_nearstation2.applymap(lambda x:pd.isnull(x)).sum())\n",
    "OW_data_final = OW_nearstation2.drop(['nearest_OW', 'n_humidity', 'n_pressure',\n",
    "       'n_temperature', 'n_wind_direction', 'n_wind_speed', 'year','week', 'day'],axis=1)\n",
    "\n",
    "###############################################################################################################################################################################################################################################################\n",
    "\n",
    "# GRID WEATHER DATA\n",
    "gw_2017 = pd.read_csv(\"gridWeather_201701-201803.csv\") \n",
    "gw_2018 = pd.read_csv(\"gridWeather_201804.csv\")\n",
    "# renaming some columns\n",
    "gw_2017 = gw_2017.rename(columns={\"utc_time\":\"time\",\"wind_speed/kph\":\"wind_speed\",\"stationName\":\"station_id\"})\n",
    "# dropping the necessary columns\n",
    "gw_2018 = gw_2018.drop(['id','weather'], axis =1)\n",
    "# appending all the GW_data\n",
    "GW_data = pd.concat([gw_2017,gw_2018], axis =0)\n",
    "GW_data = GW_data.drop([\"latitude\",\"longitude\"], axis =1)\n",
    "# converting wind speed from m/s to Km/h\n",
    "GW_data['wind_speed'] = GW_data.wind_speed*0.625\n",
    "GW_data[\"time\"] = pd.to_datetime(GW_data['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "# Checking the shape of AQ_data before removing duplicates\n",
    "print(\"Shape of GW_data before removing the duplicates:\", GW_data.shape)\n",
    "# Dropping duplicates\n",
    "GW_data.drop_duplicates(subset= None, keep= \"first\", inplace= True)\n",
    "# Sorting the entire dataframe based on station_id and time\n",
    "GW_data = GW_data.sort_values(by=['station_id', 'time'], ascending=[True,True] )\n",
    "# Checking the shape of OW_data after removing duplicates\n",
    "print(\"Shape of GW_data after removing the duplicates:\", GW_data.shape)\n",
    "\n",
    "# Performing cartesian product of GW_station and Date_hrs_train dataframes\n",
    "GW_days_stations = pd.merge(Date_hrs_train.assign(key=0), GW_station.assign(key=0), on='key').drop('key', axis=1)\n",
    "print(\"Station * Days\" + str(GW_days_stations.shape))\n",
    "# Joining the OW and days_stations dataframes\n",
    "GW_df = pd.merge(GW_data, GW_days_stations, how='right', left_on=['station_id','time'], \n",
    "                 right_on = ['station','date']).drop(['station_id','time'], axis =1)\n",
    "GW_df = GW_df.sort_values(by=['date', 'station'], ascending=[True,True] )\n",
    "\n",
    "# Making copy of new_df\n",
    "GW_nearstation = GW_df\n",
    "# Renaming all the columns of df_copy\n",
    "GW_nearstation = GW_nearstation.drop(['nearest_GW','longitude', 'latitude'], axis=1)\n",
    "GW_nearstation.rename(columns={'humidity': 'n_humidity','pressure': 'n_pressure', \"temperature\":\"n_temperature\",\n",
    "                               \"wind_direction\":\"n_wind_direction\",\"date\":\"n_date\", \"wind_speed\":\"n_wind_speed\", \n",
    "                               \"station\":\"n_station\"}, inplace=True)\n",
    "# Appending the new_df and df_copy based on nearest station and time\n",
    "GW_nearstation2 = pd.merge(GW_df, GW_nearstation, how='inner', left_on=['nearest_GW','date'], \n",
    "                           right_on = ['n_station','n_date']).drop(['n_station','n_date'], axis=1)\n",
    "# Sorting by date and station\n",
    "GW_nearstation2 = GW_nearstation2.sort_values(by=['station', 'date'], ascending=[True,True] )\n",
    "# Sorting the index\n",
    "GW_nearstation2.sort_index(inplace= True)\n",
    "\n",
    "# Filling the NaN values\n",
    "# Finding the null values in each columns\n",
    "print(\"Null values before preprocessing\\n\", GW_nearstation2.applymap(lambda x: pd.isnull(x)).sum())\n",
    "# Filling missing values with nearest station value\n",
    "GW_nearstation2['humidity'].fillna(GW_nearstation2['n_humidity'], inplace=True)\n",
    "GW_nearstation2['pressure'].fillna(GW_nearstation2['n_pressure'], inplace=True)\n",
    "GW_nearstation2['temperature'].fillna(GW_nearstation2['n_temperature'], inplace=True)\n",
    "GW_nearstation2['wind_direction'].fillna(GW_nearstation2['n_wind_direction'], inplace=True)\n",
    "GW_nearstation2['wind_speed'].fillna(GW_nearstation2['n_wind_speed'], inplace=True)\n",
    "GW_nearstation2 = ymd(GW_nearstation2)\n",
    "# filling missing values in OW data with the previous hour data of particular station, year, week\n",
    "GW_nearstation2[['humidity','pressure','temperature','wind_direction','wind_speed']] = GW_nearstation2.groupby([\"station\",\"year\",\"week\",\"hour\"])[['humidity','pressure','temperature','wind_direction','wind_speed']].transform(lambda x: x.ffill().bfill())\n",
    "# Filling missing values with mean of station , month, hour\n",
    "GW_nearstation2[['humidity','pressure','temperature','wind_direction','wind_speed']] = GW_nearstation2.groupby([\"station\",\"hour\"])[['humidity','pressure','temperature','wind_direction','wind_speed']].transform(lambda x: x.fillna(x.mean()))\n",
    "print(\"Null values after preprocessing\\n\", OW_nearstation2.applymap(lambda x:pd.isnull(x)).sum())\n",
    "GW_final = GW_nearstation2.drop(['nearest_GW', 'n_humidity', 'n_pressure',\n",
    "       'n_temperature', 'n_wind_direction', 'n_wind_speed', 'year','week', 'day'],axis=1)\n",
    "\n",
    "###############################################################################################################################################################################################################################################################\n",
    "\n",
    "# getting range of time\n",
    "print(\"start_date: \",AQ_final.date.min(),\"end_date\",AQ_final.date.max())\n",
    "print(\"start_date: \",OW_data_final.date.min(),\"end_date\",OW_data_final.date.max())\n",
    "print(\"start_date: \",GW_final.date.min(),\"end_date\",GW_final.date.max())\n",
    "# Dropping the unwanted column in AQ_final, OW_final, GW_final\n",
    "AQ = AQ_final.drop(['longitude','latitude','type','NO2','CO','SO2'], axis=1)\n",
    "OW = OW_data_final.drop(['month','hour','latitude','longitude'], axis =1)\n",
    "GW = GW_final.drop(['latitude','longitude','month','hour'], axis=1)\n",
    "df = AQ.copy()\n",
    "# merging AQ_final and OW_final based on time and stationid\n",
    "n_df = pd.merge(df, OW,  how='left', left_on=['nearest_OW','date'], \n",
    "                right_on = ['station','date'])\n",
    "# Dropping the unwanted column in n_df\n",
    "n_df = n_df.drop([\"station_y\"], axis=1)\n",
    "n_df = n_df.rename(index=str, columns={\"station_x\": \"station\"})\n",
    "# merging AQ_data and GW_data based on time and stationid\n",
    "Final_df = pd.merge(n_df, GW,  how='left', left_on=['nearest_GW','date'], \n",
    "                right_on = ['station','date']\n",
    "# Dropping the unwanted column in Final_df\n",
    "Final_df = Final_df.drop(['station_y'], axis =1)\n",
    "# Renaming the columns in Final_df\n",
    "Final_df.rename(columns={'humidity_x': 'humidity_ow','pressure_x': 'pressure_ow', \n",
    "                        \"temperature_x\":\"temperature_ow\",\"wind_direction_x\":\"wind_direction_ow\",\n",
    "                        \"wind_speed_x\":\"wind_speed_ow\", 'humidity_y': 'humidity_gw',\n",
    "                        'pressure_y': 'pressure_gw',\"temperature_y\":\"temperature_gw\",\n",
    "                        \"wind_direction_y\":\"wind_direction_gw\",\"wind_speed_y\":\"wind_speed_gw\",\n",
    "                        \"station_x\":\"station\"}, inplace=True)\n",
    "# Coping the Final_df to Final_data\n",
    "Final_data = Final_df.copy()\n",
    "# Replacing the noise with Nan\n",
    "Final_data.loc[Final_data['humidity_ow'] >100,\"humidity_ow\"]= np.nan\n",
    "Final_data.loc[Final_data['pressure_ow'] >2000,\"pressure_ow\"]= np.nan\n",
    "Final_data.loc[Final_data['temperature_ow'] > 50,\"temperature_ow\"]= np.nan\n",
    "Final_data.loc[Final_data['wind_speed_ow'] > 20,\"wind_speed_ow\"]= np.nan\n",
    "Final_data.loc[Final_data['wind_direction_ow'] > 360,\"wind_direction_ow\"]= np.nan\n",
    "# Filling NaN Values\n",
    "Final_data['humidity_ow'].fillna(Final_data[\"humidity_gw\"], inplace=True)\n",
    "Final_data['pressure_ow'].fillna(Final_data[\"pressure_gw\"], inplace=True)\n",
    "Final_data['temperature_ow'].fillna(Final_data[\"temperature_gw\"], inplace=True)\n",
    "Final_data['wind_speed_ow'].fillna(Final_data[\"wind_speed_gw\"], inplace=True)\n",
    "Final_data['wind_direction_ow'].fillna(Final_data[\"wind_direction_gw\"], inplace=True)\n",
    "# sorting the final_dataset with respect to station and date\n",
    "Final_data = Final_data.sort_values(by=['station', 'date'], ascending=[True,True] )\n",
    "# taking only the grid weather data\n",
    "X = Final_data[['station','humidity_gw','temperature_gw','wind_direction_gw','wind_speed_gw']].values\n",
    "Y = Final_data[['PM2.5','PM10','O3']].values\n",
    "#######################################################################################################################\n",
    "#######################################################################################################################\n",
    "\n",
    "# TEST DATA\n",
    "# Getting the max and min time for test data\n",
    "min_date_test = '2018-05-01 00:00:00'\n",
    "max_date_test = '2018-05-02 23:00:00'\n",
    "# Creating DATE_HRS dataframe\n",
    "Date_hrs_test = Date_hrs(min_date_test, max_date_test)\n",
    "# Performing cartesian product of OW_station and Date_hrs_train dataframes\n",
    "AQ_test_days_stations = pd.merge(Date_hrs_test.assign(key=0), AQ_station.assign(key=0), on='key').drop('key', axis=1)\n",
    "print(\"Station * Days\" + str(AQ_test_days_stations.shape))\n",
    "\n",
    "# OBSERVED WEATHE DATA\n",
    "# reading observed weather data\n",
    "OW_test = pd.read_csv(\"observedWeather_20180501-20180502.csv\")\n",
    "OW_test = OW_test.drop([\"id\",\"weather\"],axis =1)\n",
    "OW_test[\"time\"] = pd.to_datetime(OW_test['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "# Sorting the entire dataframe based on station_id and time\n",
    "OW_test = OW_test.sort_values(by=['station_id', 'time'], ascending=[True,True] )\n",
    "\n",
    "# test data\n",
    "# Getting range of time from May1 to May2\n",
    "min_date_test = OW_test.time.min()\n",
    "max_date_test = OW_test.time.max()\n",
    "\n",
    "# Creating DATE_HRS dataframe\n",
    "Date_hrs_test = Date_hrs(min_date_test, max_date_test)\n",
    "\n",
    "# Performing cartesian product of OW_station and Date_hrs_train dataframes\n",
    "OW_test_days_stations = pd.merge(Date_hrs_test.assign(key=0), OW_station.assign(key=0), on='key').drop('key', axis=1)\n",
    "print(\"Station * Days\" + str(OW_test_days_stations.shape))\n",
    "\n",
    "# Joining the OW and days_stations dataframes\n",
    "OW_df_test = pd.merge(OW_test, OW_test_days_stations, how='right', left_on=['station_id','time'], \n",
    "                 right_on = ['station','date']).drop(['station_id','time'], axis =1)\n",
    "OW_df_test = OW_df_test.sort_values(by=['date', 'station'], ascending=[True,True] )\n",
    "\n",
    "# GRID WEATHER DATA\n",
    "# reading observed weather data\n",
    "GW_test = pd.read_csv(\"gridWeather_20180501-20180502.csv\")\n",
    "GW_test = GW_test.drop([\"id\",\"weather\"],axis =1)\n",
    "GW_test[\"time\"] = pd.to_datetime(GW_test['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "# Sorting the entire dataframe based on station_id and time\n",
    "GW_test = GW_test.sort_values(by=['station_id', 'time'], ascending=[True,True] )\n",
    "# Performing cartesian product of OW_station and Date_hrs_train dataframes\n",
    "GW_test_days_stations = pd.merge(Date_hrs_test.assign(key=0), GW_station.assign(key=0), on='key').drop('key', axis=1)\n",
    "print(\"Station * Days\" + str(GW_test_days_stations.shape))\n",
    "# Joining the OW and days_stations dataframes\n",
    "GW_df_test = pd.merge(GW_test, GW_test_days_stations, how='right', left_on=['station_id','time'], \n",
    "                 right_on = ['station','date']).drop(['station_id','time'], axis =1)\n",
    "GW_df_test = GW_df_test.sort_values(by=['date', 'station'], ascending=[True,True] )\n",
    "\n",
    "# Merging all the test data\n",
    "NEW_df = pd.merge(AQ_test_days_stations, OW_test,  how='left', left_on=['nearest_OW','date'], \n",
    "                right_on = ['station_id','time'])\n",
    "Final_test_df = pd.merge(NEW_df, GW_test,  how='left', left_on=['nearest_GW','date'], \n",
    "                right_on = ['station_id','time'])\n",
    "Final_test_df = Final_test_df.drop(['longitude', 'latitude', 'type', 'nearest_AQ',\n",
    "       'nearest_OW', 'nearest_GW','station_id_x', 'time_x','station_id_y', 'time_y' ], axis =1)\n",
    "Final_test_df.rename(columns={'humidity_x': 'humidity_ow','pressure_x': 'pressure_ow', \n",
    "                        \"temperature_x\":\"temperature_ow\",\"wind_direction_x\":\"wind_direction_ow\",\n",
    "                        \"wind_speed_x\":\"wind_speed_ow\", 'humidity_y': 'humidity_gw',\n",
    "                        'pressure_y': 'pressure_gw',\"temperature_y\":\"temperature_gw\",\n",
    "                        \"wind_direction_y\":\"wind_direction_gw\",\"wind_speed_y\":\"wind_speed_gw\",\n",
    "                        \"station_x\":\"station\"}, inplace=True)\n",
    "#######################################################################################################################\n",
    "#######################################################################################################################\n",
    "# MODEL\n",
    "# One hot encoding the station data\n",
    "scaler = MinMaxScaler()\n",
    "le = LabelEncoder()\n",
    "X[:,0] = le.fit_transform(X[:,0])\n",
    "ohe = OneHotEncoder(categorical_features=[0])\n",
    "ohe_X = ohe.fit_transform(X).toarray()\n",
    "X_scaled = scaler.fit_transform(ohe_X)\n",
    "# Error Function\n",
    "def smape(actual, predicted):\n",
    "    dividend= np.abs(np.array(actual) - np.array(predicted))\n",
    "    denominator = np.array(actual) + np.array(predicted)\n",
    "    return 2 * np.mean(np.divide(dividend, denominator, out=np.zeros_like(dividend), where=denominator!=0, casting='unsafe'))\n",
    "# Use Random Forest Regressor to predict the values\n",
    "model_RF = RandomForestRegressor(random_state=42)\n",
    "model_multi_xgb = MultiOutputRegressor(XGBRegressor(max_depth=10, learning_rate=0.1, n_estimators=100, \\\n",
    "                                       silent=True, objective='reg:linear', booster='gbtree', n_jobs=-1))\n",
    "SMAPE_RF = []\n",
    "SMAPE_XG = []\n",
    "# Use K Fold Cross Validation to check the efficiency of the model\n",
    "fold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in fold.split(X_scaled):\n",
    "    x_train, x_val = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_val = Y[train_index], Y[test_index]\n",
    "    model_RF.fit(x_train, y_train)\n",
    "    preds = model_RF.predict(x_val)\n",
    "    SMAPE_RF.append(smape(y_val,preds))\n",
    "print(\"Error for random forest: \",(sum(SMAPE_RF)/len(SMAPE_RF)))\n",
    "\n",
    "# Prediction\n",
    "TEST = Final_test_df[['station','humidity_gw','temperature_gw','wind_direction_gw','wind_speed_gw']].values\n",
    "# One hot encode and normalize similair to train data\n",
    "TEST[:,0] = le.fit_transform(TEST[:,0])\n",
    "TEST_ohe = ohe.transform(TEST).toarray()\n",
    "TEST_scaled = scaler.fit_transform(TEST_ohe)\n",
    "\n",
    "predictions = model_RF.predict(TEST_scaled)\n",
    "\n",
    "test_idx = Final_test_df[['station']]\n",
    "index = list(range(0,48)) \n",
    "idx = cycle(index)\n",
    "test_idx['index'] = [next(idx) for i in range(len(test_idx))]\n",
    "test_idx['test_id'] = test_idx ['station']+'#'+test_idx['index'].astype(str)\n",
    "test_idx.drop(['index','station'],axis=1, inplace=True)\n",
    "test_idx1 = test_idx.values\n",
    "output = np.concatenate((test_idx, predictions), axis=1)\n",
    "np.savetxt('submission.csv', output, delimiter=',', header='test_id,PM2.5,PM10,O3', fmt='%s,%f,%f,%f', comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
